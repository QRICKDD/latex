We review several typical white-box adversarial attacks in this subsection.
The Fast Gradient Sign Method \textbf{(FGSM)} assumes the linear behavior in high-dimensional space is sufficient to generate adversarial inputs \cite{goodfellow2014explaining}. Therefore, it constructs adversarial samples by applying a first-order approximation of the loss function. An adversarial example can be generated within one-step update.
As an iterative version of FGSM, the Projected Gradient Descent \textbf{(PGD)} \cite{krizhevsky2012imagenet} selects the original sample as a starting point and generates adversarial examples with multiple steps. It is a powerful adversarial attack method and is therefore often used as a baseline attack for evaluating defense.
Inspired by the momentum optimizer, Dong \textit{et al.} \cite{dong2018boosting} proposed to integrate the momentum memory into the iterative process and derived a new iterative algorithm, termed momentum iterative FGSM \textbf{(MI-FGSM)}. Generally, momentum-based adversarial examples could well transfer to black-box models.
\textbf{DeepFool} is also an iterative attack method, which was proposed to generate an adversarial example with the minimum perturbation on the decision boundary of a classifier.
The Carlini \& Wagner \textbf{(CW)} attack method \cite{carlini2017towards} takes a Lagrangian form and adopts Adam \cite{kingma2015adam} for optimization.